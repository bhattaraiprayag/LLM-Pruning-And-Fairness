{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import RobertaModel, DistilBertModel, AutoModel\n",
    "import torch.nn.utils.prune as prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if GPU is available\n",
    "print(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_type, model_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    if model_path and os.path.exists(model_path):\n",
    "        # Try loading as a generic PyTorch model\n",
    "        try:\n",
    "            model = torch.load(model_path, map_location=device)\n",
    "        except Exception as e:\n",
    "            raise IOError(f\"Error loading model from {model_path}: {e}\")\n",
    "    elif model_type == 'distilroberta-base':\n",
    "        model = DistilBertModel.from_pretrained('distilroberta-base')\n",
    "    elif model_type == 'roberta-base':\n",
    "        model = RobertaModel.from_pretrained('roberta-base')\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type or path\")\n",
    "\n",
    "    return model.to(device)\n",
    "\n",
    "def check_sparsity(model):\n",
    "    total_params = 0\n",
    "    nonzero_params = 0\n",
    "    layer_sparsity = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:  # exclude non-trainable parameters\n",
    "            continue\n",
    "        layer_size = param.numel()\n",
    "        layer_nonzero = torch.count_nonzero(param)\n",
    "        layer_sparsity[name] = 1 - layer_nonzero.item() / layer_size\n",
    "        total_params += layer_size\n",
    "        nonzero_params += layer_nonzero.item()\n",
    "    overall_sparsity = 1 - nonzero_params / total_params\n",
    "    print(f\"Overall Sparsity: {overall_sparsity:.4%}\")\n",
    "    layer_sparsity_df = pd.DataFrame(layer_sparsity.items(), columns=['Layer Name', 'Sparsity'])\n",
    "    # layer_sparsity_df.sort_values(by='Sparsity', ascending=False, inplace=True)\n",
    "    display(layer_sparsity_df)\n",
    "\n",
    "def mpruner_layerwise(model, pruning_proportion):\n",
    "    # Iterate over each layer in the model\n",
    "    for name, module in model.named_modules():\n",
    "        # Check if the module is a linear layer\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            # Apply pruning to the layer\n",
    "            prune.l1_unstructured(module, name='weight', amount=pruning_proportion)\n",
    "\n",
    "    # After pruning, remove the reparametrization\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            prune.remove(module, 'weight')\n",
    "\n",
    "    # Return the pruned model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'roberta-base'     # Can be 'roberta-base', 'distilroberta-base', or a custom model path\n",
    "model_path = None               # Set this to None if you want to use pre-trained models\n",
    "\n",
    "# Loading model\n",
    "model = load_model(model_type, model_path)\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking sparsity before pruning\n",
    "check_sparsity(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruning model\n",
    "pruning_rate = 0.2              # Between 0 and 1\n",
    "pruned_model = mpruner_layerwise(model, pruning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking sparsity after pruning\n",
    "check_sparsity(pruned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "filename = f\"{model_type}-mpruned-layerwise-torch-{pruning_rate:.2f}.pt\"\n",
    "print(f\"Saving model to {filename}\")\n",
    "torch.save(pruned_model, filename)\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teamproject1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
