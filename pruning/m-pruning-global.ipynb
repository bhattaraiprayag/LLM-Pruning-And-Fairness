{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from transformers import RobertaModel, DistilBertModel, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Checking if GPU is available\n",
    "print(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_type, model_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    if model_path and os.path.exists(model_path):\n",
    "        # Try loading as a generic PyTorch model\n",
    "        try:\n",
    "            model = torch.load(model_path, map_location=device)\n",
    "        except Exception as e:\n",
    "            raise IOError(f\"Error loading model from {model_path}: {e}\")\n",
    "    elif model_type == 'distilroberta-base':\n",
    "        model = DistilBertModel.from_pretrained('distilroberta-base')\n",
    "    elif model_type == 'roberta-base':\n",
    "        model = RobertaModel.from_pretrained('roberta-base')\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type or path\")\n",
    "\n",
    "    return model.to(device)\n",
    "\n",
    "def check_sparsity(model):\n",
    "    total_params = 0\n",
    "    nonzero_params = 0\n",
    "    layer_sparsity = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:  # exclude non-trainable parameters\n",
    "            continue\n",
    "        layer_size = param.numel()\n",
    "        layer_nonzero = torch.count_nonzero(param)\n",
    "        layer_sparsity[name] = 1 - layer_nonzero.item() / layer_size\n",
    "        total_params += layer_size\n",
    "        nonzero_params += layer_nonzero.item()\n",
    "    overall_sparsity = 1 - nonzero_params / total_params\n",
    "    print(f\"Overall Sparsity: {overall_sparsity:.4%}\")\n",
    "    layer_sparsity_df = pd.DataFrame(layer_sparsity.items(), columns=['Layer Name', 'Sparsity'])\n",
    "    # layer_sparsity_df.sort_values(by='Sparsity', ascending=False, inplace=True)\n",
    "    display(layer_sparsity_df)\n",
    "\n",
    "def compute_global_threshold(model, pruning_rate, batch_size, device):\n",
    "    all_weights = [param.view(-1) for param in model.parameters() if param.requires_grad and param.dim() > 1]\n",
    "    all_weights = torch.cat(all_weights).to(device)\n",
    "    batched_quantiles = [torch.quantile(batch.abs(), pruning_rate) for batch in all_weights.split(batch_size)]\n",
    "\n",
    "    return torch.tensor(batched_quantiles).mean().to(device)\n",
    "\n",
    "def mpruner_global(model, pruning_rate, batch_size, device):\n",
    "    threshold = compute_global_threshold(model, pruning_rate, batch_size, device)\n",
    "    print(\"Global threshold:\", threshold)\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and param.dim() > 1:\n",
    "            mask = param.abs() > threshold\n",
    "            param.data.mul_(mask.to(torch.float32))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.roberta.modeling_roberta.RobertaModel'>\n"
     ]
    }
   ],
   "source": [
    "model_type = 'roberta-base'     # Can be 'roberta-base', 'distilroberta-base', or a custom model path\n",
    "model_path = None               # Set this to None if you want to use pre-trained models\n",
    "\n",
    "# Loading model\n",
    "model = load_model(model_type, model_path)\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Sparsity: 0.0019%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Layer Name</th>\n",
       "      <th>Sparsity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>embeddings.word_embeddings.weight</td>\n",
       "      <td>3.108525e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>embeddings.position_embeddings.weight</td>\n",
       "      <td>1.945525e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>embeddings.token_type_embeddings.weight</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>embeddings.LayerNorm.weight</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>embeddings.LayerNorm.bias</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>encoder.layer.11.output.dense.bias</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>encoder.layer.11.output.LayerNorm.weight</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>encoder.layer.11.output.LayerNorm.bias</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>pooler.dense.weight</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>pooler.dense.bias</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Layer Name      Sparsity\n",
       "0           embeddings.word_embeddings.weight  3.108525e-07\n",
       "1       embeddings.position_embeddings.weight  1.945525e-03\n",
       "2     embeddings.token_type_embeddings.weight  1.000000e+00\n",
       "3                 embeddings.LayerNorm.weight  0.000000e+00\n",
       "4                   embeddings.LayerNorm.bias  0.000000e+00\n",
       "..                                        ...           ...\n",
       "194        encoder.layer.11.output.dense.bias  0.000000e+00\n",
       "195  encoder.layer.11.output.LayerNorm.weight  0.000000e+00\n",
       "196    encoder.layer.11.output.LayerNorm.bias  0.000000e+00\n",
       "197                       pooler.dense.weight  0.000000e+00\n",
       "198                         pooler.dense.bias  1.000000e+00\n",
       "\n",
       "[199 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checking sparsity before pruning\n",
    "check_sparsity(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global threshold: tensor(0.0167, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Pruning model\n",
    "pruning_rate = 0.2              # Between 0 and 1\n",
    "batch_size = 5000\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pruned_model = mpruner_global(model, pruning_rate, batch_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Sparsity: 24.8828%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Layer Name</th>\n",
       "      <th>Sparsity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>embeddings.word_embeddings.weight</td>\n",
       "      <td>0.118609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>embeddings.position_embeddings.weight</td>\n",
       "      <td>0.293711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>embeddings.token_type_embeddings.weight</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>embeddings.LayerNorm.weight</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>embeddings.LayerNorm.bias</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>encoder.layer.11.output.dense.bias</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>encoder.layer.11.output.LayerNorm.weight</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>encoder.layer.11.output.LayerNorm.bias</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>pooler.dense.weight</td>\n",
       "      <td>0.597107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>pooler.dense.bias</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Layer Name  Sparsity\n",
       "0           embeddings.word_embeddings.weight  0.118609\n",
       "1       embeddings.position_embeddings.weight  0.293711\n",
       "2     embeddings.token_type_embeddings.weight  1.000000\n",
       "3                 embeddings.LayerNorm.weight  0.000000\n",
       "4                   embeddings.LayerNorm.bias  0.000000\n",
       "..                                        ...       ...\n",
       "194        encoder.layer.11.output.dense.bias  0.000000\n",
       "195  encoder.layer.11.output.LayerNorm.weight  0.000000\n",
       "196    encoder.layer.11.output.LayerNorm.bias  0.000000\n",
       "197                       pooler.dense.weight  0.597107\n",
       "198                         pooler.dense.bias  1.000000\n",
       "\n",
       "[199 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checking sparsity after pruning\n",
    "check_sparsity(pruned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to roberta-base-mpruned-global-0.20.pt\n"
     ]
    }
   ],
   "source": [
    "# Saving the model\n",
    "filename = f\"{model_type}-mpruned-global-{pruning_rate:.2f}.pt\"\n",
    "print(f\"Saving model to {filename}\")\n",
    "torch.save(pruned_model, filename)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teamproject1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
